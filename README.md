# Data Analysis Project: NYC Taxi Customer Data Analysis

## Motivation

As a data analyst student, I designed this project to practice the full data analysis workflow—from data collection to visualization and reporting—using real-world data. My goal was to develop hands-on skills with modern data tools, apply best practices in transparency and reproducibility, and generate actionable insights. Leveraging AWS cloud services was a key part of my approach, enabling scalable, collaborative, and secure data handling throughout the project.

## Tech Stack

- **Python**: Core language for data manipulation and analysis.
- **Jupyter Notebook**: Interactive environment for developing and documenting the workflow.
- **Pandas**: Data cleaning, transformation, and exploratory analysis.
- **Matplotlib & Seaborn**: Data visualization.
- **NumPy**: Numerical operations.
- **AWS (Amazon Web Services)**:
  - **Amazon S3**: Centralized, secure storage of raw and processed datasets.
  - **AWS Lambda**: Automated data ingestion and preprocessing.
  - **Amazon EC2**: Scalable compute environment for running Jupyter Notebooks and analysis.
  - **AWS IAM**: Managed permissions for team collaboration.
- **Git & GitHub**: Version control, project collaboration, and sharing results.
- **Markdown**: Documentation and reporting.

## Project Workflow

### 1. Problem Definition & Planning
- Defined the analytical question and project objectives.
- Evaluated AWS services for storage, compute, and automation needs.

### 2. Data Collection
- Sourced datasets from reputable sources.
- Uploaded all data to **Amazon S3** for secure, scalable storage.

### 3. Data Cleaning & Preprocessing
- Automated preprocessing (format conversion, validation) using **AWS Lambda**.
- Cleaned data with Pandas: handled missing values, standardized types, and converted dates.

### 4. Exploratory Data Analysis (EDA)
- Ran **Jupyter Notebooks** on **Amazon EC2** for scalable analysis.
- Used Pandas, Matplotlib, and Seaborn for descriptive statistics and visualizations.

### 5. Feature Engineering
- Created new features (e.g., extracting weekday/year from dates)[3].
- Selected relevant features based on EDA.

### 6. Data Analysis & Modeling
- Applied analytical techniques to address the research question.
- Used EC2 resources for model training and evaluation.
- Interpreted results in context.

### 7. Results Visualization & Reporting
- Summarized findings in charts and tables.
- Documented the process in Jupyter and Markdown for transparency.
- Stored final reports and visualizations in **Amazon S3**.

### 8. Version Control & Collaboration
- Used Git for version tracking.
- Published code and documentation on GitHub.
- Managed AWS permissions with **IAM** for secure collaboration.

### 9. Project Review & Reflection
- Assessed outcomes versus objectives.
- Identified limitations and suggested future improvements, including further AWS automation and scaling.

## Outcomes

- Completed a full data analysis pipeline using AWS for cloud-based storage, compute, and automation.
- Produced actionable insights and visualizations addressing the initial question.
- Developed a reproducible, transparent workflow aligned with best practices.
- Enhanced skills in Python, data analysis libraries, AWS, and version control.
- Demonstrated the integration of cloud computing into data analytics for scalable, real-world projects.
